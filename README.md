# Language Modeling From Scratch

This ongoing project builds Transformer-based language models (LM) from scratch using PyTorch, following the course content of Stanford CS336. 

The repo implements the building blocks for a LM: 
- Byte-pair encoding (BPE) tokenizer
- A decoder-only Transformer model with MultiHeadSelfAttention
- AdamW optimizer
- Training pipeline
